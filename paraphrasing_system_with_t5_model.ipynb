{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "paraphrasing_system_with_t5_model.ipynb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets evaluate sacrebleu nltk --quiet\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words(\"turkish\"))"
      ],
      "metadata": {
        "id": "WsVSQPOZXUoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "x7a8C7tBXUko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load with correct separator (Excel often uses semicolons)\n",
        "df = pd.read_csv(\"cleaned_paraphrasing_dataset.csv\")\n",
        "\n",
        "# Rename columns to standard names\n",
        "df = df.rename(columns={\"Asil\": \"input\", \"Parafraz\": \"target\"})\n",
        "\n",
        "# Strip extra quotes if needed\n",
        "df[\"input\"] = df[\"input\"].str.strip('\"')\n",
        "df[\"target\"] = df[\"target\"].str.strip('\"')"
      ],
      "metadata": {
        "id": "VqUEDUZaXUiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to Hugging Face Dataset\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# Train/test split\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n"
      ],
      "metadata": {
        "id": "QQcsvyxqXUf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "K8z2QzQcXUdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "wjsARbv3XUae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "vE80jx23XkJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Veri setinin boyutu:\", df.shape)"
      ],
      "metadata": {
        "id": "r6U5sSquXUYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where input or target is missing\n",
        "dataset = dataset.filter(lambda x: x[\"input\"] is not None and x[\"target\"] is not None)\n",
        "dataset = dataset.filter(lambda x: x[\"input\"].strip() != \"\" and x[\"target\"].strip() != \"\")\n"
      ],
      "metadata": {
        "id": "aH9feS0tXUVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "df = df.dropna()\n",
        "df = df[['input', 'target']]\n",
        "\n",
        "dataset = Dataset.from_pandas(df)\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n"
      ],
      "metadata": {
        "id": "bJAReUnNXUSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = df.dropna()"
      ],
      "metadata": {
        "id": "vXFi5PbcXUP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Temizlenmiş df'in varsa:\n",
        "dataset = Dataset.from_pandas(df[[\"input\", \"target\"]])\n",
        "\n",
        "# Eğitim/doğrulama ayırımı\n",
        "train_test = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = train_test[\"train\"]\n",
        "val_dataset = train_test[\"test\"]"
      ],
      "metadata": {
        "id": "4a3cERQ8XUNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
        "\n",
        "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
        "model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n",
        "\n",
        "# Doğrudan raw veriyi kullanıyoruz\n",
        "def tokenize_function(examples):\n",
        "    inputs = [\"parafraze et: \" + text for text in examples[\"input\"]]\n",
        "    model_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"target\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "    # Replace padding token id's in labels with -100\n",
        "    labels[\"input_ids\"] = [\n",
        "        [(token if token != tokenizer.pad_token_id else -100) for token in label]\n",
        "        for label in labels[\"input_ids\"]\n",
        "    ]\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n"
      ],
      "metadata": {
        "id": "O2Ay7l5xXUK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCV_rv9fXTqS"
      },
      "outputs": [],
      "source": [
        "# ✅ Tokenları görselleştir\n",
        "def visualize_tokenization(example_input):\n",
        "    text = \"paraphrase: \" + example_input\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    print(\"\\n🧾 Input Text:\", text)\n",
        "    print(\"🔤 Tokens:\", tokens)\n",
        "    print(\"🔢 Token IDs:\", token_ids)\n",
        "\n",
        "# Örnek veri üzerinde test et\n",
        "visualize_tokenization(train_dataset[0][\"input\"])\n",
        "\n",
        "# Dataset'i tokenize et\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_val = val_dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 📌 Aynı input ve target'a sahip satırları bul\n",
        "duplicate_rows = df[df[\"input\"] == df[\"target\"]]\n",
        "\n",
        "# 🔁 Bu satırların sayısını yazdır\n",
        "print(\"🔁 Aynı input ve target'a sahip satır sayısı:\", len(duplicate_rows))\n",
        "\n",
        "# ✅ Bu satırları veri setinden çıkar\n",
        "df_cleaned = df[df[\"input\"] != df[\"target\"]]\n",
        "\n",
        "# 📊 Kalan toplam satır sayısını yazdır\n",
        "print(\"✅ Temizlenmiş veri setindeki toplam satır sayısı:\", len(df_cleaned))\n"
      ],
      "metadata": {
        "id": "PAgmJd2_X3H2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MT5ForConditionalGeneration, MT5Tokenizer, TrainingArguments, Trainer\n",
        "\n",
        "# ✅ Use multilingual mT5 model\n",
        "model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n",
        "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./mt5_paraphraser_tr\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=3e-4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    logging_dir=\"./logs\",\n",
        "    fp16=False\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer\n",
        ")\n"
      ],
      "metadata": {
        "id": "Nwb6l1r3X40L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.model.to(\"cpu\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "fKsGa_HpX7JG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔄 Switch model to CPU to prevent CUDA OutOfMemory errors\n",
        "import torch\n",
        "trainer.model.to(\"cpu\")\n",
        "\n",
        "# ✅ Take a small sample from the validation set\n",
        "small_eval_set = tokenized_val.select(range(1000))  # adjust size if needed\n",
        "\n",
        "# ✅ Perform safe inference without Trainer.evaluate()\n",
        "from tqdm import tqdm\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "model = trainer.model\n",
        "tokenizer = trainer.tokenizer\n",
        "model.eval()\n",
        "\n",
        "for sample in tqdm(small_eval_set):\n",
        "    input_text = \"paraphrase: \" + sample[\"input\"]\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(input_ids, max_length=128, num_beams=5)\n",
        "\n",
        "    pred = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    predictions.append(pred.strip())\n",
        "    references.append(sample[\"target\"].strip())\n"
      ],
      "metadata": {
        "id": "DXRAxe_4X9T9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score\n",
        "import evaluate\n",
        "import torch, gc\n",
        "\n",
        "# ✅ Compute BLEU and ROUGE scores\n",
        "import evaluate\n",
        "bleu = evaluate.load(\"sacrebleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "bleu_score = bleu.compute(predictions=predictions, references=[[r] for r in references])\n",
        "rouge_score = rouge.compute(predictions=predictions, references=references)\n",
        "\n",
        "print(\"✅ Evaluation Complete\")\n",
        "print(\"BLEU Score:\", bleu_score[\"score\"])\n",
        "print(\"ROUGE-L Score:\", rouge_score[\"rougeL\"])\n"
      ],
      "metadata": {
        "id": "rS_XoYmVX_s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Authenticate first (only once per session)\n",
        "login()  # paste token from huggingface.co/settings/tokens\n"
      ],
      "metadata": {
        "id": "N8FGa43zYHmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "model_path = \"/content/mt5_paraphraser_tr/checkpoint-2649\"\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "\n",
        "def paraphrase(text, num_return_sequences=1):\n",
        "    input_text = f\"parafraze et: {text}\"\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128).input_ids\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        temperature=0.9,\n",
        "        max_length=128,\n",
        "        num_return_sequences=num_return_sequences\n",
        "    )\n",
        "    return [tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jNqfAfnQYKGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "print(paraphrase(\"Film gerçekten çok güzeldi.\"))"
      ],
      "metadata": {
        "id": "KSu5K_djYQOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(paraphrase(\"bugün güzel bir gün geçirdim.\"))"
      ],
      "metadata": {
        "id": "SQZ5Pa0DzKpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(paraphrase(\"Uçakla yolculuk yapmayı seviyorum.\"))"
      ],
      "metadata": {
        "id": "V9SzRwjtzO-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(paraphrase(\"kitap okumak insanın bilgisini arttırır.\"))"
      ],
      "metadata": {
        "id": "PxQJWid7zgel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(os.path.exists(\"/content/mt5_paraphraser_tr/checkpoint-2649\"))\n"
      ],
      "metadata": {
        "id": "1978W3ewvmgp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}